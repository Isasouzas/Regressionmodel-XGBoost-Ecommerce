# -*- coding: utf-8 -*-
"""Cópia de projeto_infinity._XGBoot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QbV8F5cNzrOyiE6hjOkVYbLZHAVJx99O
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ======================================================================
# 1. CARREGAMENTO E PRÉ-PROCESSAMENTO DOS DADOS
# ======================================================================
"""
Nesta primeira seção, realizamos a carga dos dados e todas as transformações necessárias:
- Conversão de tipos de dados
- Cálculo do valor total das vendas
- Tratamento de valores missing
- Filtragem de valores inconsistentes
- Remoção de colunas não relevantes
"""

df_completo = pd.read_csv('/content/data.csv',encoding='latin1')

db = df_completo.iloc[1:15001]

db.head()

db.info()

db['InvoiceDate'] = pd.to_datetime(db['InvoiceDate'])

db['Sales_Amount'] = db['Quantity'] * db['UnitPrice']

# Tratamento de dados
db['CustomerID'] = db['CustomerID'].fillna(18239.0)  # Preenche clientes missing com o ID mais frequente
db_modificado = db[db['Quantity'] > 0]  # Remove quantidades negativas
db_final = db_modificado[db_modificado['UnitPrice'] > 0]  # Remove preços negativos
db_final = db_final.drop(['InvoiceDate'], axis=1)  # Remove coluna temporal

# ======================================================================
# 2. ANÁLISE EXPLORATÓRIA DE DADOS (EDA)
# ======================================================================
"""
Aqui exploramos os dados para entender:
- Estatísticas básicas das vendas
- Distribuição dos valores
- Produtos mais vendidos
- Vendas por país
Essa análise ajuda a identificar padrões e anomalias nos dados.
"""
import matplotlib.pyplot as plt
import seaborn as sns

print("\n=== ANÁLISE EXPLORATÓRIA ===")
print(f"Média de vendas: USD {db_final['Sales_Amount'].mean():.2f}")
print(f"Valor mínimo de venda: USD {db_final['Sales_Amount'].min():.2f}")
print(f"Valor máximo de venda: USD {db_final['Sales_Amount'].max():.2f}")

# Visualização da distribuição de vendas
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(data=db_final, x='Sales_Amount', kde=True, bins=50)
plt.title('Distribuição do Valor das Vendas')

# Identificação do produto mais vendido
quantidade_por_produto = db_final.groupby('Description')['Quantity'].sum().reset_index()
produto_mais_vendido = quantidade_por_produto.sort_values(by='Quantity', ascending=False).iloc[0]
print(f"\nProduto mais vendido: '{produto_mais_vendido['Description']}' com {produto_mais_vendido['Quantity']} unidades")

# Análise de vendas por país
vendas_por_pais = db_final.groupby('Country')['Sales_Amount'].sum().reset_index()
vendas_por_pais = vendas_por_pais.sort_values(by='Sales_Amount', ascending=False)

plt.subplot(1, 2, 2)
sns.barplot(data=vendas_por_pais, x='Country', y='Sales_Amount', palette='viridis')
plt.xticks(rotation=90)
plt.title('Vendas por País')
plt.tight_layout()
plt.show()



# ======================================================================
# 3. PREPARAÇÃO DOS DADOS PARA MODELAGEM
# ======================================================================
"""
Nesta etapa:
- Separamos os dados em features (X) e target (y)
- Dividimos em conjuntos de treino e teste
- Definimos as colunas numéricas e categóricas
"""
from sklearn.model_selection import train_test_split

X = db_final[['InvoiceNo', 'StockCode','CustomerID','Country','Quantity','UnitPrice']]
y = db_final['Sales_Amount']

# Divisão treino-teste (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)

# ======================================================================
# 4. CONSTRUÇÃO E TREINAMENTO DO MODELO XGBOOST
# ======================================================================
"""
Implementamos um pipeline de machine learning com:
- Pré-processamento: normalização de numéricas e one-hot encoding de categóricas
- Modelo XGBoost com parâmetros otimizados
- Early stopping para evitar overfitting
"""
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb

# Definição das features
numeric_features = ['Quantity','UnitPrice']
categorical_features = ['CustomerID', 'InvoiceNo','StockCode','Country']

# Pré-processamento
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ])

# Pipeline sem early stopping (removemos os parâmetros relacionados)
xgb_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=200,  # Reduzimos pois não temos early stopping
        learning_rate=0.03,
        max_depth=7,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=99,
        eval_metric='mae'
    ))
])

# Treinamento simples
print("\n=== TREINAMENTO DO MODELO ===")
xgb_model.fit(X_train, y_train)

# Predição
y_pred = xgb_model.predict(X_test)

# ======================================================================
# 5. AVALIAÇÃO DO MODELO
# ======================================================================
"""
Avaliamos o modelo usando múltiplas métricas:
- MSE (Mean Squared Error)
- MAE (Mean Absolute Error)
- R² (Coeficiente de Determinação)
"""
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_pred_xgb = xgb_model.predict(X_test)

print("\n=== MÉTRICAS DE PERFORMANCE ===")
print(f'XGBoost MSE: {mean_squared_error(y_test, y_pred_xgb):.2f}')
print(f'XGBoost MAE: {mean_absolute_error(y_test, y_pred_xgb):.2f}')
print(f'XGBoost R²: {r2_score(y_test, y_pred_xgb):.2f}')

# ======================================================================
# 6. VISUALIZAÇÕES E INTERPRETAÇÃO DO MODELO
# ======================================================================
"""
Criamos visualizações para:
1. Importância das variáveis
2. Análise de resíduos
3. Comparação entre valores reais e preditos
"""
plt.figure(figsize=(15, 5))

# Gráfico de importância de features
plt.subplot(1, 3, 1)
xgb_reg = xgb_model.named_steps['regressor']
xgb.plot_importance(xgb_reg, max_num_features=10, ax=plt.gca())
plt.title('Importância das Features')

# Gráfico de resíduos
plt.subplot(1, 3, 2)
residuals = y_test - y_pred_xgb
plt.scatter(y_pred_xgb, residuals, alpha=0.3)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Análise de Resíduos')
plt.xlabel('Valores Preditos')
plt.ylabel('Resíduos (Real - Predito)')

# Comparação Real vs Predito
plt.subplot(1, 3, 3)
plt.scatter(y_test, y_pred_xgb, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.title('Real vs Predito')
plt.xlabel('Valor Real')
plt.ylabel('Valor Predito')

plt.tight_layout()
plt.show()

# ======================================================================
# 7. CONCLUSÕES E INSIGHTS FINAIS
# ======================================================================
"""
Principais conclusões do projeto:
1. O modelo XGBoost mostrou excelente performance na previsão de valores de venda
2. As features mais importantes foram [listar as top 3 do gráfico]
3. O modelo tem maior dificuldade em prever valores extremamente altos
4. A análise por país revelou oportunidades de expansão de mercado
"""

print("\n=== INSIGHTS FINAIS ===")
print("3. O modelo explica {:.1f}% da variância nos dados (R²)".format(r2_score(y_test, y_pred_xgb)*100))
print("4. O erro médio absoluto foi de USD {:.2f}".format(mean_absolute_error(y_test, y_pred_xgb)))
print('Esse erro é relativamente baixo, já que a média da coluna Sales_Amount é de USD 18.')